import sys

sys.path = [
    p for p in sys.path if "/.local/lib" not in p
]  # Use conda env installation of duckdb

import duckdb
import pandas as pd
import matplotlib
import os
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(style="white", context="talk")
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
import matplotlib.cm as cm
from pathlib import Path

from trebl_tools import plotting
from trebl_tools.preprocess import time_it
from trebl_tools.error_correct import run_whitelist_on_concat_domains
from trebl_tools import error_correct

class MapRefiner:
    """Refine and filter sequencing maps generated by InitialMapper.
    
    This class provides a flexible pipeline for processing barcode mapping tables
    through multiple sequential filtering steps including quality control, error
    correction, read count thresholding, and target uniqueness filtering.
    
    Args:
        db_path (str): Path to the DuckDB database containing mapping tables.
        bc_objects (list[Barcode]): List of barcode objects with extraction parameters.
        column_pairs (list[tuple]): List of (key_columns, target_columns) tuples
            for unique target filtering. Each element can be a string or list of strings.
        reads_threshold (int): Minimum read count threshold for filtering.
        map_order (list[str], optional): Custom order of processing steps. If None,
            prompts user to select from default steps. Defaults to None.
        step_name (str, optional): Identifier for this processing step. Defaults to "".
        descriptor (str, optional): Additional descriptor for table naming. Defaults to "".
        design_file (str, optional): Path to design file for validation. Defaults to None.
        output_figures_path (str, optional): Directory for saving plots and outputs. 
            Defaults to None.
        min_fraction_major_target (float, optional): Minimum fraction of reads that
            must come from the most abundant target. Defaults to 0.9.
        manual_ec_threshold (bool, optional): Whether to manually set error correction
            threshold instead of automatic detection. Defaults to True.
        umi_object (object, optional): UMI extraction object. Defaults to None.
            
    Example:
        >>> # Define barcode objects and column pairs
        >>> ad_bc = Barcode(name="AD_BC", preceder="ATCG", post="GCTA", length=20)
        >>> rep_bc = Barcode(name="REP_BC", preceder="TTAA", post="GGCC", length=15)
        >>> pairs = [("AD_BC", "REP_BC"), (["AD_BC", "REP_BC"], "target")]
        >>> 
        >>> # Create refiner with custom processing order
        >>> refiner = MapRefiner(
        ...     db_path="experiment.duckdb",
        ...     bc_objects=[ad_bc, rep_bc],
        ...     column_pairs=pairs,
        ...     reads_threshold=10,
        ...     map_order=["quality", "designed", "grouped", "thresholded"],
        ...     step_name="step2",
        ...     output_figures_path="results/"
        ... )
        >>> 
        >>> # Execute refinement pipeline
        >>> refiner.refine_map_from_db()
        >>> 
        >>> # Generate loss summary
        >>> loss_df = refiner.save_loss_table()
        >>> refiner.plot_loss()
        
    Note:
        Processing steps are applied sequentially. Each step creates a new table
        based on the previous step's output. Table names follow the pattern:
        {step_name}_{barcode_names}_{descriptor}_{step_suffix}
    """

    DEFAULT_MAP_ORDER = [
        "initial",
        "quality",
        "error_corrected",   # new step
        "grouped",
        "thresholded",
        "barcode_exists",       
        "unique_target",
        "designed"
    ]


    def __init__(self, 
                 db_path, 
                 bc_objects, 
                 column_pairs, 
                 reads_threshold,
                 map_order=None,
                 step_name="", 
                 descriptor="", 
                 design_file = None, 
                 output_figures_path=None, 
                 min_fraction_major_target = 0.9,  
                 manual_ec_threshold = True, 
                 umi_object = None):
        """Initialize MapRefiner with processing parameters and database connection.
        
        Sets up database connection, configures table naming, and prompts user
        for processing step order if not provided.
        
        Args:
            db_path (str): Path to DuckDB database.
            bc_objects (list): Barcode objects for processing.
            column_pairs (list): Key-target column pairs for filtering.
            reads_threshold (int): Minimum read count threshold.
            map_order (list, optional): Custom processing step order.
            step_name (str): Processing step identifier.
            descriptor (str): Additional table name descriptor.
            design_file (str, optional): Path to validation design file.
            output_figures_path (str, optional): Output directory for plots.
            min_fraction_major_target (float): Fraction threshold for target filtering.
            manual_ec_threshold (bool): Use manual error correction threshold.
            umi_object (object, optional): UMI extraction object.
        """
        self.db_path = db_path
        self.con = duckdb.connect(self.db_path)
        self.bc_objects = [bc for bc in bc_objects]
        self.cols = [bc.name for bc in self.bc_objects]
        self.column_pairs = column_pairs
        self.reads_threshold = reads_threshold
        self.step_name = step_name
        self.descriptor = descriptor
        self.design_file = design_file
        self.output_figures_path = output_figures_path
        self.min_fraction_major_target = min_fraction_major_target
        self.manual_ec_threshold = manual_ec_threshold
        self.umi_object = umi_object

        cols_str = "_".join(self.cols)
        self.table_prefix = f"{step_name}_{cols_str}_"
        
        # --- Build a stable base prefix that never changes across descriptors ---
        # Example: step1_AD_ADBC_RPBC_
        cols_str = "_".join(self.cols)
        if step_name:
            self.table_prefix_base = f"{step_name}_{cols_str}_"
        else:
            self.table_prefix_base = f"{cols_str}_"
    
        # --- Optionally append descriptor later ---
        # This one changes per run if you specify a descriptor
        # e.g. "step1_AD_ADBC_RPBC_strict_"
        if descriptor:
            self.table_prefix_with_descriptor = f"{self.table_prefix_base}{descriptor}_"
        else:
            self.table_prefix_with_descriptor = self.table_prefix_base
    
        print("Base prefix (stable across descriptors):", self.table_prefix_base)
        print("Full prefix for this instance:", self.table_prefix_with_descriptor)
        print()

        valid_steps = [step for step in self.DEFAULT_MAP_ORDER if step != "initial"]

        # Always start with 'initial'
        if map_order is None:
             # --- Map order setup ---
            print("Default map order:")
            for i, name in enumerate(valid_steps, 1):
                print(f"{i}. {name}")
            print()
            
            user_input = input(
                f"Enter map order by numbers for steps after 'initial', comma-separated (e.g., 1,3,2), or press Enter for default: "
            )
            if user_input.strip() == "":
                self.map_order = ["initial"] + valid_steps
            else:
                try:
                    indices = [int(x.strip()) - 1 for x in user_input.split(",")]
                    chosen = [valid_steps[i] for i in indices]
                    self.map_order = ["initial"] + chosen
                except Exception as e:
                    print(f"Invalid input ({e}), using default order.")
                    self.map_order = ["initial"] + valid_steps
        else:
            invalid = [step for step in map_order if step not in valid_steps]
            if invalid:
                raise ValueError(f"Invalid map step(s) in custom order: {invalid}")
            self.map_order = ["initial"] + map_order

        print("Using the following step order:")
        for i, name in enumerate(self.map_order, 1):
            print(f"{i}. {name}")
        print()


    def show_tables(self):
        """Display all tables in the connected DuckDB database.
        
        Returns:
            list: List of tuples containing table names in the database.
            
        Example:
            >>> tables = refiner.show_tables()
            >>> print(f"Found {len(tables)} tables in database")
        """
        tables = self.con.execute("SHOW TABLES").fetchall()
        # for table in tables:
        #     print(table)
        return list(tables)
    
    def _prefixed(self, table_name):
        """Generate properly prefixed table names based on processing context.
        
        Creates table names with appropriate prefixes depending on the table type
        and processing context. Initial tables use base prefix, while processed
        tables use descriptor prefix.
        
        Args:
            table_name (str): Base table name without prefix.
            
        Returns:
            str: Fully prefixed table name.
            
        Note:
            - 'initial' always uses base prefix
            - 'grouped' uses base prefix if following 'initial', otherwise descriptor prefix  
            - All other tables use descriptor prefix
            
        Example:
            >>> prefixed_name = refiner._prefixed("quality")
            >>> print(prefixed_name)
            step1_AD_BC_REP_BC_strict_quality
        """
        if table_name == "initial":
            return f"{self.table_prefix_base}{table_name}"
    
        elif table_name == "grouped":
            # Determine the index of "grouped" and check what comes before it
            if "grouped" in self.map_order:
                idx = self.map_order.index("grouped")
                prev_step = self.map_order[idx - 1] if idx > 0 else None
            else:
                prev_step = None
    
            # If grouped follows "initial", use base prefix (no descriptor)
            if prev_step == "initial":
                return f"{self.table_prefix_base}initial_grouped"
            else:
                return f"{self.table_prefix_with_descriptor}{table_name}"
    
        else:
            # All other tables use descriptor prefix
            return f"{self.table_prefix_with_descriptor}{table_name}"


    # -------------------------------
    # Table creation functions
    # -------------------------------

    def check_exists(self, check_table_name):
        """Check if a table exists and determine if processing should be skipped.
        
        Args:
            check_table_name (str): Full table name to check for existence.
            
        Returns:
            bool: True if table exists and processing should be skipped, False otherwise.
            
        Note:
            Only skips for initial, grouped, or loss tables to avoid overwriting
            important base tables. Controlled by should_check_exists attribute.
        """
        if self.should_check_exists:
            # Get list of existing tables
            existing_tables = [t[0] for t in self.con.execute("SHOW TABLES").fetchall()]
            
            # Only skip if table exists AND ends with "_initial" or "_initial_grouped"
            if check_table_name in existing_tables and (
                check_table_name.endswith("_initial") or check_table_name.endswith("_initial_grouped") or check_table_name.endswith("_loss_table")
            ):
                print(f"Skipping — table {check_table_name} already exists and is initial/grouped.")
                return True
            else:
                return False
        else:
            return False
            
    @time_it
    def create_initial(self, parquet_path):
        """Load initial mapping table from Parquet file into database.
        
        Creates the initial mapping table by reading from a Parquet file.
        This serves as the starting point for all subsequent processing steps.
        
        Args:
            parquet_path (str): Path to the Parquet file containing initial mapping data.
            
        Note:
            Skips loading if table already exists and check_exists is enabled.
            
        Example:
            >>> refiner.create_initial("initial_mapping.parquet")
            Reading initial map from initial_mapping.parquet as step1_AD_BC_initial...
            Done in 5.23 seconds.
        """
        print(f"Reading initial map from {parquet_path} as {self._prefixed('initial')}...")
        if not self.check_exists(self._prefixed('initial')):
            self.con.execute(f"""
                CREATE OR REPLACE TABLE {self._prefixed('initial')} AS
                SELECT *
                FROM read_parquet('{parquet_path}')
            """)

    @time_it
    def create_quality(self, previous_table="initial"):
        """Filter to high-quality reads based on barcode and UMI quality flags.
        
        Keeps only rows where all barcode quality flags (and UMI quality flag
        if present) are TRUE, indicating successful extraction with expected lengths.
        
        Args:
            previous_table (str, optional): Name of the previous table to filter.
                Defaults to "initial".
                
        Note:
            Automatically includes UMI quality filtering if umi_object is configured.
            
        Example:
            >>> refiner.create_quality("initial") 
            Filtering to high-quality reads...
            Created table: step1_AD_BC_REP_BC_quality — filtered for TRUE in all *_qual columns.
            Done in 12.45 seconds.
        """
        print("\nFiltering to high-quality reads...")
    
        # Start with barcode *_qual columns
        qual_columns = [f"{c}_qual = TRUE" for c in self.cols]
        
        # Add UMI *_qual if umi_object exists
        if self.umi_object is not None:
            qual_columns.append(f"{self.umi_object.name}_qual = TRUE")
        
        # Combine into WHERE clause
        where_clause = " AND ".join(qual_columns)
    
        self.con.execute(f"""
            CREATE OR REPLACE TABLE {self._prefixed('quality')} AS
            SELECT *
            FROM {self._prefixed(previous_table)}
            WHERE {where_clause};
        """)
    
        print(f"Created table: {self._prefixed('quality')} — filtered for TRUE in all *_qual columns.")
    
    @time_it
    def create_designed(self, previous_table="quality"):
        """Filter to designed sequences only.
        
        Keeps only rows where the Designed column equals 1, indicating that
        the barcode combination matches the designed library.
        
        Args:
            previous_table (str, optional): Name of the previous table to filter.
                Defaults to "quality".
            
        Example:
            >>> refiner.create_designed("quality")
            Filtering to designed sequences...
            Created table: step1_AD_BC_REP_BC_designed — kept only Designed == 1.
            Done in 8.67 seconds.
        """
        print("\nFiltering to designed sequences...")
    
        self.con.execute(f"""
            CREATE OR REPLACE TABLE {self._prefixed('designed')} AS
            SELECT *
            FROM {self._prefixed(previous_table)}
            WHERE CAST(Designed AS INTEGER) = 1;
        """)
    
        print(f"Created table: {self._prefixed('designed')} — kept only Designed == 1.")

    @time_it
    def barcode_exists(self, previous_table="initial"):
        """Remove rows with null or empty barcode sequences.
        
        Filters out rows where any barcode column (except AD) is NULL or
        contains only whitespace, ensuring all required barcodes are present.
        
        Args:
            previous_table (str, optional): Name of the previous table to filter.
                Defaults to "initial".
            
        Example:
            >>> refiner.barcode_exists("initial")
            Removing rows with null or empty barcodes (excluding AD)...
            Done in 15.23 seconds.
        """
        print("Removing rows with null or empty barcodes (excluding AD)...")
        prev_table = self._prefixed(previous_table)
        output_table = self._prefixed("barcode_exists")
    
        # Filter columns: exclude 'AD'
        barcode_cols = [c for c in self.cols if c != "AD"]
    
        if barcode_cols:
            # Build condition to detect NULL or empty strings in barcode columns
            null_empty_condition = " OR ".join([f"{c} IS NULL OR TRIM({c}) = ''" for c in barcode_cols])
            where_clause = f"WHERE NOT ({null_empty_condition})"
        else:
            # No barcode columns to filter
            where_clause = ""
    
        self.con.execute(f"""
            CREATE OR REPLACE TABLE {output_table} AS
            SELECT *
            FROM {prev_table}
            {where_clause}
        """)
    
    @time_it
    def create_grouped(self, previous_table="quality_designed"):
        """Group identical barcode combinations and count occurrences.
        
        Aggregates reads by unique barcode combinations, creating a count column
        with the number of occurrences for each combination. Optionally generates
        a read count distribution histogram.
        
        Args:
            previous_table (str, optional): Name of the previous table to group.
                Defaults to "quality_designed".
                
        Note:
            Groups by all barcode columns plus quality and design flags.
            Generates histogram plot if output_figures_path is specified.
            
        Example:
            >>> refiner.create_grouped("quality")
            Grouping step1_AD_BC_REP_BC_quality...
            Done in 45.67 seconds.
        """
        print(f"Grouping {self._prefixed(previous_table)}...")

        grouped_table_name = self._prefixed('grouped')
        group_cols_sql = ", ".join(self.cols)
        
        if not self.check_exists(grouped_table_name):
            qual_cols = [f"{c}_qual" for c in self.cols]
            qual_cols_sql = ", ".join(qual_cols + ["Designed"])
            self.con.execute(f"""
                CREATE OR REPLACE TABLE {grouped_table_name} AS
                SELECT {group_cols_sql},
                    COUNT(*) AS count,
                    {qual_cols_sql}
                FROM {self._prefixed(previous_table)}
                GROUP BY {group_cols_sql}, {qual_cols_sql}
                ORDER BY count DESC
            """)

        if self.output_figures_path:
            sns.set_style('ticks')
            fig, ax = plt.subplots(figsize = (5,2.5), dpi = 300)
            self.plot_reads_histogram(grouped_table_name, ax = ax, edgecolor = 'none', bins = 100)
            plt.title("Grouped " + group_cols_sql)
            
            grouped_table_name = self._prefixed('grouped')
            filename = os.path.join(self.output_figures_path, f"{grouped_table_name}.png")
            plt.savefig(filename, bbox_inches="tight")

            plt.show()

    @time_it
    def create_thresholded(self, previous_table):
        """Filter barcode combinations by minimum read count threshold.
        
        Removes barcode combinations with read counts below the specified threshold.
        Prompts user for threshold if not already set. Optionally generates a
        histogram showing the threshold cutoff.
        
        Args:
            previous_table (str): Name of the previous table to threshold.
            
        Note:
            Uses self.reads_threshold or prompts user for input.
            Generates histogram with threshold line if output_figures_path is specified.
            
        Example:
            >>> refiner.create_thresholded("grouped")
            Thresholding...
            Using reads threshold of 10.
            Done in 8.34 seconds.
        """

        print("Thresholding...")
        if not self.reads_threshold:    
            try:
                user_input = input("Enter minimum read count threshold (default = 5): ")
                self.reads_threshold = int(user_input) if user_input.strip() != "" else 5
            except ValueError:
                print("Invalid input. Using default threshold of 5.")
                self.reads_threshold = 5
        
        print("Using reads threshold of " + str(self.reads_threshold) + ".")

        if self.output_figures_path:
            fig, ax = plt.subplots(figsize = (5,2.5), dpi = 300)
            self.plot_reads_histogram(previous_table, ax = ax, edgecolor = 'none', bins = 100)
            ax.axvline(self.reads_threshold, color = 'red')
            
            grouped_table_name = self._prefixed('grouped')
            filename = os.path.join(self.output_figures_path, f"{self._prefixed('thresholded')}.png")
            plt.savefig(filename, bbox_inches="tight")

            plt.show()

        self.con.execute(f"""
            CREATE OR REPLACE TABLE {self._prefixed('thresholded')} AS
            SELECT *
            FROM {self._prefixed(previous_table)}
            WHERE count > {self.reads_threshold}
        """)

    # ERR CORRECT
    @time_it
    def generate_fastq_for_whitelist(self, output_dir=None, suffix="_barcodes_extracted.fastq", prev_table = "quality"):
        """Generate synthetic FASTQ file from filtered barcode sequences for error correction.
        
        Creates a FASTQ file containing concatenated barcode sequences for input
        to UMI-tools whitelist error correction. Only includes reads that pass
        all quality checks.
        
        Args:
            output_dir (str or Path, optional): Directory to save FASTQ. 
                Defaults to current directory.
            suffix (str, optional): Suffix for FASTQ filename. 
                Defaults to "_barcodes_extracted.fastq".
            prev_table (str, optional): Name of the previous table to use. 
                Defaults to "quality".
                
        Returns:
            str: Path to the generated FASTQ file.
            
        Note:
            Adds dummy UMI 'A' and quality scores 'I' for UMI-tools compatibility.
            Only includes reads where all barcode quality flags are TRUE.
            
        Example:
            >>> fastq_path = refiner.generate_fastq_for_whitelist("temp/", prev_table="quality")
            Generating FASTQ: temp/step1_AD_BC_filtered_barcodes_extracted.fastq
            Wrote 50000 reads to temp/step1_AD_BC_filtered_barcodes_extracted.fastq
            Done in 23.45 seconds.
        """
        con = self.con
    
        # Resolve output directory
        if output_dir is None:
            output_dir = Path(".")
        else:
            output_dir = Path(output_dir).resolve()
            output_dir.mkdir(exist_ok=True)
    
        # Construct output FASTQ path
        output_fastq = output_dir / f"{self.table_prefix}filtered{suffix}"
        print(f"Generating FASTQ: {output_fastq}")
    
        # Get all barcode and quality column names
        bc_cols = self.cols
        qual_cols = [f"{name}_qual" for name in bc_cols]
    
        # Build quality filter (require all barcodes to pass length check)
        qual_filter = " AND ".join([f"{qc} = TRUE" for qc in qual_cols])
    
        # Construct concatenation expression for barcodes (+ optional UMI)
        concat_expr = " || ".join(bc_cols)
    
        # Use previous table
        table_name = prev_table 
    
        # Query the concatenated reads that pass quality checks
        query = f"""
            SELECT {concat_expr} AS sequence
            FROM {table_name}
            WHERE {qual_filter};
        """
        seq_df = con.execute(query).df()
    
        # Construct FASTQ records
        with open(output_fastq, "w") as f:
            for i, seq in enumerate(seq_df["sequence"], start=1):
                # Dummy FASTQ format: 4 lines per record
                # Adding "A" as dummy UMI
                f.write(f"@read_{i}\nA{seq}\n+\n{'I' * (len(seq) + 1)}\n")
    
        print(f"Wrote {len(seq_df)} reads to {output_fastq}")
        return str(output_fastq)
    
    def apply_whitelist(self, output_dir, prev_table=None):
        """Apply UMI-tools whitelist error correction to barcode sequences.
        
        Generates FASTQ from barcode sequences, runs UMI-tools whitelist to identify
        canonical sequences, and updates the database with error-corrected barcodes.
        Recalculates quality flags and design validation after correction.
        
        Args:
            output_dir (str or Path): Directory for temporary files and whitelist outputs.
            prev_table (str, optional): Name of the previous table to correct.
                
        Note:
            Either uses manual threshold (prompts user) or automatic detection for
            expected barcode count. Updates barcode sequences in-place with canonical
            forms. Removes temporary FASTQ file after processing.
            
        Example:
            >>> refiner.apply_whitelist("error_correction/", prev_table="quality")
            === Applying whitelist for step1 ===
            Generating FASTQ: error_correction/step1_AD_BC_filtered_barcodes_extracted.fastq
            Running umi_tools whitelist...
            Unique canonical barcodes: 1500
            Whitelist application complete for step1 at step1_AD_BC_error_corrected
        """
        output_dir = Path(output_dir).resolve()
        output_dir.mkdir(exist_ok=True)
    
        print(f"\n=== Applying whitelist for {self.step_name} ===")
    
        table_name = prev_table
        new_table = self._prefixed("error_corrected")
    
        # Generate FASTQ for whitelist
        fastq_path = self.generate_fastq_for_whitelist(output_dir, prev_table=table_name)
        fastq_path = Path(fastq_path)
    
        # expected_whitelist = output_dir / f"{fastq_path.stem}_whitelist.txt"
        # expected_log = output_dir / f"{fastq_path.stem}_whitelist.log"
    
        # # Run or reuse whitelist
        # if expected_whitelist.exists() and expected_log.exists():
        #     print(f"Whitelist already exists, skipping:\n  {expected_whitelist}")
        #     whitelist_path = expected_whitelist
        
        # else:
            
        # Infer expected_bc_count if needed
        if self.manual_ec_threshold:
    
            print("Inferring expected barcode count from grouped reads...")
    
            # Build grouped counts from prev_table
            group_cols_sql = ", ".join(self.cols)
            tmp_grouped = "__tmp_bc_grouped"
    
            self.con.execute(f"""
                CREATE OR REPLACE TABLE {tmp_grouped} AS
                SELECT {group_cols_sql}, COUNT(*) AS count
                FROM {table_name}
                GROUP BY {group_cols_sql}
            """)
    
            # Prompt for reads threshold if needed (same behavior as create_thresholded)
            if not self.reads_threshold:
                if self.output_figures_path:
                    # fig, ax = plt.subplots(figsize=(5, 2.5), dpi=300)
                    # self.plot_reads_histogram(tmp_grouped, ax = ax, edgecolor = 'none', bins = 100)
                    fig, ax = plt.subplots(figsize=(5, 2.5), dpi=300)
                    df_tmp = self.con.execute(f"SELECT count FROM {tmp_grouped}").df()
                    sns.histplot(df_tmp["count"], bins=100, ax=ax, log_scale = (True, True))
                    plt.title("Grouped barcode read counts")
                    plt.show()

                    filename = os.path.join(self.output_figures_path, f"{tmp_grouped}.png")
                    plt.savefig(filename, bbox_inches="tight")
    
                try:
                    user_input = input("Enter minimum read count threshold (default = 5): ")
                    self.reads_threshold = int(user_input) if user_input.strip() != "" else 5
                except ValueError:
                    print("Invalid input. Using default threshold of 5.")
                    self.reads_threshold = 5
    
            print(f"Using reads threshold of {self.reads_threshold}")
    
            # Count how many groups exceed threshold
            expected_bc_count = self.con.execute(f"""
                SELECT COUNT(*)
                FROM {tmp_grouped}
                WHERE count > {self.reads_threshold}
            """).fetchone()[0]
    
            print(f"Inferred expected barcode count: {expected_bc_count}")
    
            self.con.execute(f"DROP TABLE IF EXISTS {tmp_grouped}")

            print("Running umi_tools whitelist...")
            whitelist_outputs = run_whitelist_on_concat_domains(
                fastq_path=fastq_path,
                output_dir=output_dir,
                prefix=None,
                set_cell_number=expected_bc_count
            )
            whitelist_path = whitelist_outputs["whitelist"]


        else:
            print(f"Using automatic detection of expected barcode count.")

            print("Running umi_tools whitelist...")
            whitelist_outputs = run_whitelist_on_concat_domains(
                fastq_path=fastq_path,
                output_dir=output_dir,
                prefix=None
            )
            whitelist_path = whitelist_outputs["whitelist"]

    
        
        # Load whitelist mapping
        mapping_df = error_correct.convert_txt_to_whitelist_mapping_df_from_path(whitelist_path)
        print(f"Unique canonical barcodes: {mapping_df['canonical'].nunique()}")
    
        self.con.execute("CREATE OR REPLACE TABLE barcode_map AS SELECT * FROM mapping_df;")
    
        # Join + replace barcodes
        concat_expr = " || ".join(self.cols)
        self.con.execute(f"""
            CREATE OR REPLACE TABLE {new_table} AS
            SELECT t.*, b.canonical AS concat_canonical
            FROM {table_name} AS t
            JOIN barcode_map AS b
            ON {concat_expr} = b.original
        """)
    
        start_idx = 0
        for bc in self.bc_objects:
            self.con.execute(f"""
                UPDATE {new_table}
                SET {bc.name} = SUBSTR(concat_canonical, {start_idx + 1}, {bc.length})
            """)
            self.con.execute(f"""
                UPDATE {new_table}
                SET {bc.name}_qual = LENGTH({bc.name}) = {bc.length}
            """)
            start_idx += bc.length
    
        # Recalculate Designed
        self.con.execute(f"""
            ALTER TABLE {new_table} DROP COLUMN IF EXISTS Designed;
        """)
        self.merge_design(prev_table=new_table)
    
        self.con.execute(f"""
            ALTER TABLE {new_table} DROP COLUMN concat_canonical;
        """)

        os.remove(fastq_path) # Removing temp fastq file created
        
        print(f"Whitelist application complete for {self.step_name} at {new_table}")
    
    @time_it
    def merge_design(self, prev_table=None):
        """Merge with design file to validate barcode combinations.
        
        Adds or updates the 'Designed' column by comparing barcode sequences
        against a design file. Sets Designed=1 for sequences that match the
        design library, Designed=0 otherwise.
        
        Args:
            prev_table (str, optional): Name of the table to update. 
                Defaults to "seq".
                
        Note:
            If no design file is provided, sets all sequences as designed (Designed=1).
            Automatically removes the 'sequence' column if present to save space.
            
        Example:
            >>> refiner.merge_design(prev_table="error_corrected")
            Merging with design file...
            Done in 12.34 seconds.
        """
        con = self.con
        table_to_use = prev_table or "seq"
        target_table = table_to_use  # output table is now the same as input table
    
        if self.design_file:
            print("Merging with design file...")
            # Load design file
            con.execute(f"""
                CREATE OR REPLACE TABLE design AS
                SELECT CAST(column0 AS VARCHAR) AS AD
                FROM read_csv_auto('{self.design_file}', header=False)
            """)
            # Merge design info into the table
            con.execute(f"""
                CREATE OR REPLACE TABLE {target_table} AS
                SELECT t.*, CASE WHEN d.AD IS NOT NULL THEN 1 ELSE 0 END AS Designed
                FROM {table_to_use} AS t
                LEFT JOIN design AS d USING(AD);
            """)
        else:
            # No design file, default Designed = 1
            con.execute(f"""
                CREATE OR REPLACE TABLE {target_table} AS
                SELECT *, 1 AS Designed
                FROM {table_to_use};
            """)
    
        # Drop sequence column if present
        columns = [col for col in con.execute(f"PRAGMA table_info('{target_table}')").fetchall()]
        if any(c[1] == 'sequence' for c in columns):
            con.execute(f"ALTER TABLE {target_table} DROP COLUMN sequence;")
    
    @time_it
    def create_error_corrected(self, previous_table="quality"):
        """Perform error correction using UMI-tools whitelist approach.
        
        Executes the complete error correction pipeline: generates FASTQ from
        barcode sequences, applies UMI-tools whitelist algorithm, and updates
        the database with canonical (error-corrected) barcode sequences.
        
        Args:
            previous_table (str, optional): Name of the previous table to correct.
                Defaults to "quality".
                
        Note:
            Creates output directory for intermediate files if not exists.
            All temporary files are cleaned up automatically.
            
        Example:
            >>> refiner.create_error_corrected("quality")
            === Running error correction step on step1_AD_BC_quality ===
            Generating FASTQ: error_corrected/step1_AD_BC_filtered_barcodes_extracted.fastq
            Running umi_tools whitelist...
            Whitelist application complete for step1 at step1_AD_BC_error_corrected  
            Done in 2.5 minutes.
        """
        print(f"\n=== Running error correction step on {self._prefixed(previous_table)} ===")
        output_dir = self.output_figures_path or "./error_corrected"
        os.makedirs(output_dir, exist_ok=True)
        
        # Generate FASTQ + run whitelist-based correction
        try:
            self.apply_whitelist(output_dir, prev_table = self._prefixed(previous_table))
            
        except Exception as e:
            print(f"Error correction failed: {e}")

    @time_it
    def create_unique_target(self, previous_table):
        """Filter for unique target relationships using configurable column pairs.
        
        For each (key_columns, target_columns) pair, keeps only keys where the
        most abundant target accounts for at least min_fraction_major_target
        of reads. This ensures that each key maps predominantly to a single target.
        
        Args:
            previous_table (str): Name of the previous table to filter.
            
        Returns:
            str: Name of the created filtered table.
            
        Note:
            Processes each column pair independently then intersects results.
            Uses self.min_fraction_major_target as the threshold (default 0.9).
            Cleans up intermediate tables automatically.
            
        Example:
            >>> # With column_pairs = [("AD_BC", "REP_BC"), (["AD_BC", "REP_BC"], "target")]
            >>> filtered_table = refiner.create_unique_target("thresholded")
            Filtering so that at least 90% of reads come from the most abundant target...
            Processing mapping 1: AD_BC → REP_BC  
            Processing mapping 2: AD_BC || '-' || REP_BC → target
            Created filtered table: step1_AD_BC_REP_BC_unique_target
            Done in 15.67 seconds.
        """
        frac = self.min_fraction_major_target
        print(f"Filtering so that at least {frac*100:.0f}% of reads come from the most abundant target...")
    
        base_table = self._prefixed(previous_table)
        filter_tables = []
    
        # Step 1: build independent filters for each (key, target) mapping
        for i, (key_cols, target_cols) in enumerate(self.column_pairs):
            tmp_name = self._prefixed(f"unique_target_step_{i}")
            filter_tables.append(tmp_name)
    
            # Build key/target expressions
            key_expr = (
                key_cols if isinstance(key_cols, str)
                else " || '-' || ".join([f"{c}" for c in key_cols])
            )
            target_expr = (
                target_cols if isinstance(target_cols, str)
                else " || '-' || ".join([f"{c}" for c in target_cols])
            )
    
            print(f"\tProcessing mapping {i+1}: {key_expr} → {target_expr}")
    
            query = f"""
                CREATE OR REPLACE TABLE {tmp_name} AS
                WITH counts AS (
                    SELECT 
                        {key_expr} AS computed_key,
                        {target_expr} AS computed_target,
                        SUM(count) AS cnt
                    FROM {base_table}
                    GROUP BY computed_key, computed_target
                ),
                totals AS (
                    SELECT 
                        computed_key,
                        SUM(cnt) AS total_cnt,
                        MAX(cnt) AS max_cnt
                    FROM counts
                    GROUP BY computed_key
                )
                SELECT computed_key
                FROM totals
                WHERE CAST(max_cnt AS FLOAT) / total_cnt >= {frac};
            """
    
            self.con.execute(query)
    
        # Step 2: intersect all filter tables
        final_name = self._prefixed("unique_target")
        final_conditions = []
    
        for tbl, (key_cols, _) in zip(filter_tables, self.column_pairs):
            key_expr = (
                key_cols if isinstance(key_cols, str)
                else " || '-' || ".join([f"{c}" for c in key_cols])
            )
            final_conditions.append(f"{key_expr} IN (SELECT computed_key FROM {tbl})")
    
        query_final = f"""
            CREATE OR REPLACE TABLE {final_name} AS
            SELECT *
            FROM {base_table} AS base
            WHERE {" AND ".join(final_conditions)};
        """
    
        self.con.execute(query_final)
    
        # Step 3: optional cleanup of intermediate tables
        for tbl in filter_tables:
            self.con.execute(f"DROP TABLE IF EXISTS {tbl};")
    
        print(f"Created filtered table: {final_name}")
        return final_name


    # -------------------------------
    # Pipelines
    # -------------------------------

    def refine_map_from_db(self, save_name = None, should_check_exists = False):
        """Execute the complete map refinement pipeline.
        
        Runs all configured processing steps in sequence according to map_order.
        Each step creates a new table based on the previous step's output.
        
        Args:
            save_name (str, optional): Name for final output table. If provided,
                creates a copy of the final result with this name. Defaults to None.
            should_check_exists (bool, optional): Whether to skip processing if
                tables already exist. Defaults to False.
                
        Note:
            Steps are executed in the order specified by self.map_order.
            Each processing function receives the previous table name as input.
            
        Example:
            >>> refiner.refine_map_from_db(save_name="final_refined_map")  
            # Executes: initial -> quality -> designed -> grouped -> thresholded
            Done.
        """

        self.should_check_exists = should_check_exists
        
        table_to_func = {
            "initial": None,
            "quality": self.create_quality,
            "designed": self.create_designed,
            "barcode_exists": self.barcode_exists,
            "grouped": self.create_grouped,
            "error_corrected": self.create_error_corrected,  # new
            "thresholded": self.create_thresholded,
            "unique_target": self.create_unique_target
        }

        prev_table = "initial"
        for table_name in self.map_order:
            func = table_to_func.get(table_name)
            if func is not None:
                func(previous_table=prev_table)
            prev_table = table_name

        if save_name:
            self.con.execute(f"CREATE OR REPLACE TABLE {save_name} AS SELECT * FROM {self._prefixed(prev_table)}")
        print("Done.")
            
    # -------------------------------
    # Table access / saving
    # -------------------------------
    def get_map_df(self, map_name, insert_prefix = True):
        """Retrieve a mapping table as a pandas DataFrame.
        
        Loads a table from the database, automatically handling prefixed and
        non-prefixed table names.
        
        Args:
            map_name (str): Base name of the table to retrieve.
            insert_prefix (bool, optional): Whether to try prefixed name first.
                Defaults to True.
                
        Returns:
            pd.DataFrame: DataFrame containing the requested table data.
            
        Example:
            >>> df = refiner.get_map_df("quality")  
            >>> print(f"Retrieved {len(df)} rows from quality table")
        """
        existing_tables = [t[0] for t in self.con.execute("SHOW TABLES").fetchall()]
        prefixed_name = self._prefixed(map_name)
        if prefixed_name not in existing_tables:
            return self.con.execute(f"SELECT * FROM {map_name}").df()
        else:
            return self.con.execute(f"SELECT * FROM {prefixed_name}").df()

    def save_map(self, map_name, map_path):
        """Save a mapping table to CSV file.
        
        Exports a table from the database to a CSV file with headers.
        
        Args:
            map_name (str): Name of the table to save.
            map_path (str): Path where CSV file should be saved.
            
        Example:
            >>> refiner.save_map("quality", "outputs/quality_filtered.csv")
        """
        self.con.execute(f"COPY {self._prefixed(map_name)} TO '{map_path}' WITH (HEADER, DELIMITER ',')")

    @time_it
    def save_loss_table(self):
        """Generate and save comprehensive processing loss summary.
        
        Creates a detailed summary table showing read counts, unique sequences,
        and percentage losses at each step of the refinement pipeline. Includes
        both step-by-step and cumulative loss calculations.
        
        Returns:
            pd.DataFrame: Summary DataFrame with columns:
                - map (str): Processing step name
                - description (str): Human-readable step description  
                - unique_count (int): Number of unique barcode combinations
                - unique_AD_count (int): Number of unique AD sequences
                - total_reads (int): Total number of reads (if count column exists)
                - % of previous step (float): Percentage retained from previous step
                - % of total reads (float): Percentage retained from initial step
                
        Note:
            Automatically saves CSV file if output_figures_path is configured.
            Also creates a database table with the loss summary for further analysis.
            Step descriptions are defined in the internal map_info_dict.
            
        Example:
            >>> loss_df = refiner.save_loss_table()
            >>> print(loss_df)
                    map                        description  unique_count  total_reads  % of previous step
            0   initial              Initial combinations         10000       100000              100.00
            1   quality           After quality filtering          8000        80000               80.00  
            2  designed           After designed filtering          7500        75000               93.75
            3   grouped                    Grouped counts          7500        75000              100.00
            4 thresholded  Filtered by # reads > 10              5000        65000               66.67
            Saved loss summary table as 'step1_AD_BC_REP_BC_strict_loss_summary'
        """
        # Mapping step descriptions
        map_info_dict = {
            "initial": "Initial combinations",
            "grouped": "Grouped counts",
            "thresholded": f"Filtered by # reads > {self.reads_threshold}",
            "unique_target": "Filtered for unique targets",
            "quality": "After quality filtering",
            "designed": "After designed filtering",

        }
    
        lengths = []
        existing_tables = [t[0] for t in self.con.execute("SHOW TABLES").fetchall()]
    
        for table_name in self.map_order:
            prefixed_name = self._prefixed(table_name)
            if prefixed_name in existing_tables:
                # Get unique count
                unique_count = self.con.execute(f"SELECT COUNT(*) FROM {prefixed_name}").fetchone()[0]
                
                # Get column info
                columns = [col[0] for col in self.con.execute(f"DESCRIBE {prefixed_name}").fetchall()]
                
                # Total reads
                total_reads = self.con.execute(f"SELECT SUM(count) FROM {prefixed_name}").fetchone()[0] if "count" in columns else unique_count
                
                # Unique AD count
                unique_AD_count = self.con.execute(f"SELECT COUNT(DISTINCT AD) FROM {prefixed_name}").fetchone()[0] if "AD" in columns else 0
            else:
                # Table missing
                unique_count = None
                total_reads = None
                unique_AD_count = None
    
            # Append info for this step
            description = map_info_dict.get(table_name, table_name)
            lengths.append({
                "map": table_name,
                "description": description,
                "unique_count": unique_count,
                "unique_AD_count": unique_AD_count,
                "total_reads": total_reads
            })
    
        # Build DataFrame
        df = pd.DataFrame(lengths)
    
        # Compute percentages
        prev_count = None
        map1_count = df.loc[df['map'] == 'initial', 'unique_count'].values[0] if not df.empty else None
        percent_prev, percent_map1 = [], []
    
        for count in df['unique_count']:
            # % vs previous step
            percent_prev.append(round(100 * count / prev_count, 2) if count is not None and prev_count is not None else None)
            # % vs initial
            percent_map1.append(round(100 * count / map1_count, 2) if count is not None and map1_count is not None else None)
            prev_count = count if count is not None else prev_count
    
        df['% of previous step'] = pd.Series(percent_prev)
        df['% of total reads'] = pd.Series(percent_map1)
    
        # Fill NaNs in percentage columns only
        df['% of previous step'] = df['% of previous step'].fillna(100)
        df['% of total reads'] = df['% of total reads'].fillna(100)
    
        # Save CSV if requested
        if self.output_figures_path:
            output_csv_path = os.path.join(self.output_figures_path, f"{self.table_prefix_with_descriptor}loss_summary.csv")
            df.to_csv(output_csv_path, index=False)
        
        # Save summary table to SQL (overwrite if exists)
        loss_table_name = self._prefixed("loss_summary")
        self.con.execute(f"DROP TABLE IF EXISTS {loss_table_name}")
        self.con.execute(f"CREATE TABLE {loss_table_name} AS SELECT * FROM df")
    
        print(f"Saved loss summary table as '{loss_table_name}'")
        
        return df

    # -------------------------------
    # Plotting functions
    # -------------------------------
    def plot_loss(self, ax=None, palette="rocket_r", text_offset = 0, show_background = True):
        """Create visualization of processing losses across refinement steps.
        
        Generates a horizontal bar plot showing both total reads and unique counts
        at each processing step, with options for background bars and count labels.
        
        Args:
            ax (matplotlib.axes.Axes, optional): Existing axis to plot on. 
                If None, creates new figure. Defaults to None.
            palette (str, optional): Seaborn color palette name. Defaults to "rocket_r".
            text_offset (float, optional): Vertical offset for count labels. 
                Defaults to 0.
            show_background (bool, optional): Whether to show background bars for
                total reads with reduced opacity. Defaults to True.
                
        Returns:
            tuple: (fig, ax) matplotlib Figure and Axes objects.
            
        Example:
            >>> fig, ax = refiner.plot_loss(palette="viridis", show_background=True)
            >>> plt.title("Processing Loss Summary")
            >>> plt.show()
            
        Note:
            Automatically calls save_loss_table() to generate summary data.
            Saves plot automatically if output_figures_path is configured.
        """
        # Name of saved summary table
        loss_table_name = self._prefixed("loss_summary")
    
        # # Use check_exists() and get_map_df() to get the DataFrame
        # if self.check_exists(loss_table_name):
        #     df = self.get_map_df("loss_summary")
        # else:
        df = self.save_loss_table()

        return plotting.plot_loss_helper(ax=ax, 
                                         palette=palette, 
                                         text_offset = text_offset, 
                                         show_background = show_background,
                                         default_map_order = self.DEFAULT_MAP_ORDER, 
                                         output_figures_path = self.output_figures_path,
                                         table_prefix_with_descriptor=self.table_prefix_with_descriptor, 
                                         df=df)
        
    def plot_reads_histogram(self, previous_table, save_path=None, ax=None, **kwargs):
        """Generate histogram of read count distributions for a processing table.
        
        Creates a log-log scale histogram showing the distribution of read counts
        for barcode combinations in the specified table.
        
        Args:
            previous_table (str): Name of the table to plot.
            save_path (str, optional): Path to save the plot. Defaults to None.
            ax (matplotlib.axes.Axes, optional): Existing axis to plot on. 
                Defaults to None.
            **kwargs: Additional arguments passed to the plotting function.
                
        Returns:
            matplotlib.axes.Axes: Axis object with the histogram.
            
        Example:
            >>> ax = refiner.plot_reads_histogram("grouped", bins=50, alpha=0.7)
            >>> plt.title("Read Count Distribution")
            >>> plt.show()
        """
        map_df = self.get_map_df(previous_table)
        return plotting.plot_reads_histogram(map_df, save_path=save_path, ax=ax, **kwargs)

    def close_connection(self):
        """Close the DuckDB database connection.
        
        Properly closes the database connection and sets it to None to prevent
        further operations on a closed connection.
        
        Example:
            >>> refiner.close_connection()
        """
        if self.con:
            self.con.close()
            self.con = None

    @time_it
    def plot_error_correction(self, save_dir=None, plot=True):
        """Generate summary plots and analysis of error correction results.
        
        Creates comprehensive visualizations of barcode error correction performance
        by reading whitelist files and generating summary statistics and plots.
        
        Args:
            save_dir (str or Path, optional): Directory to save plot outputs.
                If None, uses output_figures_path. Defaults to None.
            plot (bool, optional): Whether to generate and save plots. 
                Defaults to True.
                
        Returns:
            pd.DataFrame: Summary table with error correction statistics including
                canonical sequences, collapsed sequences, and group sizes.
                
        Note:
            Delegates to plotting.plot_error_correction() for implementation.
            
        Example:
            >>> summary_df = refiner.plot_error_correction(plot=True)
            >>> print(f"Error correction reduced {len(summary_df)} sequence groups")
            Done in 45.23 seconds.
        """
        return plotting.plot_error_correction(self.output_figures_path,
                                              self.table_prefix_with_descriptor,
                                              save_dir,
                                              plot)

    @time_it
    def plot_all_whitelists_from_summary(self, summary_df, n_cols=4, dpi=300):
        """Create multi-panel visualization of error correction summaries.
        
        Generates a comprehensive figure with multiple panels showing error
        correction performance metrics using pre-computed summary data.
        
        Args:
            summary_df (pd.DataFrame): DataFrame with error correction summary data.
                Must contain columns: ['canonical', 'num_merged', 'largest_count', 'rest_count'].
            n_cols (int, optional): Number of panels per row. Defaults to 4.
            dpi (int, optional): Figure resolution in dots per inch. Defaults to 300.
                
        Returns:
            matplotlib.figure.Figure: Figure object containing all summary panels.
            
        Note:
            Delegates to plotting.plot_all_whitelists_from_summary() for implementation.
            
        Example:
            >>> summary_df = refiner.plot_error_correction(plot=False)
            >>> fig = refiner.plot_all_whitelists_from_summary(summary_df)
            >>> fig.savefig("whitelist_analysis.png", bbox_inches="tight")
            Done in 12.34 seconds.
        """
        return plotting.plot_all_whitelists_from_summary(summary_df, n_cols, dpi)

