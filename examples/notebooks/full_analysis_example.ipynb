{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREBL Full Analysis Example\n",
    "\n",
    "This notebook demonstrates a comprehensive TREBL analysis workflow with:\n",
    "- **Error correction enabled** (improves accuracy by salvaging sequences)\n",
    "- **Both simple and directional/complex UMI deduplication** (comprehensive PCR artifact removal)\n",
    "\n",
    "This approach is recommended for final, publication-quality analysis when accuracy is the priority.\n",
    "\n",
    "**Note:** Error correction and complex UMI deduplication significantly increase processing time. For large datasets, consider submitting as a Savio job (see `examples/savio_jobs/full_analysis_job.sh`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trebl_tools import (\n",
    "    initial_map,\n",
    "    map_refiner,\n",
    "    complexity,\n",
    "    finder,\n",
    "    preprocess,\n",
    "    error_correct,\n",
    "    plotting,\n",
    "    umi_deduplicate,\n",
    "    pipelines\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pipeline\n",
    "\n",
    "Key settings for full analysis:\n",
    "- `error_correction=True` - Enables UMI-tools based error correction\n",
    "- `test_n_reads` - Optional: Set to a number for testing with subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline with error correction\n",
    "pipeline = pipelines.TreblPipeline(\n",
    "    db_path=\"full_analysis.db\",\n",
    "    design_file_path=\"path/to/your/design_file.txt\",  # Update this path\n",
    "    error_correction=True,  # Enable error correction for full analysis\n",
    "    output_path=\"output/full_analysis\"\n",
    "    # test_n_reads=100000  # Uncomment to test with first 100k reads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: TREBL Mapping with Error Correction\n",
    "\n",
    "Define barcodes and run initial mapping with error correction enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define barcodes to search for in reads\n",
    "AD = finder.Barcode(\n",
    "    name=\"AD\",\n",
    "    preceder=\"GGCTAGC\",\n",
    "    post=\"TGACTAG\",\n",
    "    length=120\n",
    ")\n",
    "\n",
    "AD_BC = finder.Barcode(\n",
    "    name=\"AD_BC\",\n",
    "    preceder=\"CGCGCC\",\n",
    "    post=\"GGGCCC\",\n",
    "    length=11\n",
    ")\n",
    "\n",
    "RT_BC = finder.Barcode(\n",
    "    name=\"RT_BC\",\n",
    "    preceder=\"CTCGAG\",\n",
    "    post=\"GGCCGC\",\n",
    "    length=14\n",
    ")\n",
    "\n",
    "# Combine barcodes\n",
    "bc_objects = [AD, AD_BC, RT_BC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify sequencing file(s)\n",
    "step1_seq_file = \"path/to/your/step1_sequencing_file.fastq\"  # Update this path\n",
    "# Can be a single file (string) or multiple files (list of strings)\n",
    "# Supported formats: .fastq or .fastq.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reads distribution\n",
    "# NOTE: For large files (>10M reads), consider submitting this as a Savio job\n",
    "# See examples/savio_jobs/full_analysis_job.sh for job submission example\n",
    "\n",
    "pipeline.step1_reads_distribution(\n",
    "    seq_file=step1_seq_file,\n",
    "    bc_objects=bc_objects,\n",
    "    reverse_complement=True\n",
    ")\n",
    "# Produces histogram of reads per barcode\n",
    "# Helps pick appropriate reads_threshold for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Step 1 mapping with error correction\n",
    "# Error correction happens automatically based on pipeline initialization\n",
    "step1_map = pipeline.run_step_1(\n",
    "    seq_file=step1_seq_file,\n",
    "    bc_objects=bc_objects,\n",
    "    column_pairs=[(\"RT_BC\", \"AD\")],  # Check for collisions between RT_BC and AD\n",
    "    reads_threshold=10,  # Minimum reads to keep a barcode\n",
    "    reverse_complement=False\n",
    ")\n",
    "# Returns DataFrame of Step 1 mapping\n",
    "# Error correction will salvage sequences similar to high-read sequences\n",
    "# Saves CSV, loss table visualization, and optional loss table CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: TREBL Step 2 Mapping\n",
    "\n",
    "Step 2 processes AD and RT libraries that are now in separate sequencing files.\n",
    "\n",
    "**Note:** Step 1 must be completed successfully before running Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing files for AD and RT (Step 2)\n",
    "step2_AD_seq_file = \"path/to/your/step2_AD_file.fastq\"  # Update this path\n",
    "step2_RT_seq_file = \"path/to/your/step2_RT_file.fastq\"  # Update this path\n",
    "# Can be single files or lists of files; .fastq or .fastq.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Step 2 reads distribution\n",
    "# NOTE: For large files (>10M reads), consider submitting this as a Savio job\n",
    "\n",
    "pipeline.step2_reads_distribution(\n",
    "    AD_seq_file=step2_AD_seq_file,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_file=step2_RT_seq_file,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True\n",
    ")\n",
    "# Produces histograms for AD and RT reads\n",
    "# Helps pick reads_threshold_AD and reads_threshold_RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Step 2 mapping with error correction\n",
    "# Error correction happens automatically based on pipeline initialization\n",
    "step2 = pipeline.run_step_2(\n",
    "    AD_seq_file=step2_AD_seq_file,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_file=step2_RT_seq_file,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True,\n",
    "    reads_threshold_AD=10,\n",
    "    reads_threshold_RT=10,\n",
    "    step1_map_csv_path=\"output/full_analysis/step1_AD_AD_BC_RT_BC_error_corrected_designed.csv\"  # Update with your step1 CSV path\n",
    ")\n",
    "\n",
    "# Extract outputs\n",
    "AD_step2 = step2[\"AD_step2\"]\n",
    "RT_step2 = step2[\"RT_step2\"]\n",
    "step1_overlap = step2[\"step1_overlap\"]\n",
    "\n",
    "# The overlap shows how well Step 2 data aligns with Step 1 mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREBL Experiment with Both UMI Deduplication Methods\n",
    "\n",
    "Process the full TREBL experiment using both simple and directional/complex UMI deduplication.\n",
    "\n",
    "**UMI Deduplication Methods:**\n",
    "- **Simple:** Counts unique UMI sequences (faster, good baseline)\n",
    "- **Directional/Complex:** Uses UMI-tools directional algorithm to account for PCR and sequencing errors in UMIs (more accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UMI objects\n",
    "AD_UMI = finder.Barcode(\n",
    "    name=\"UMI\",\n",
    "    preceder=\"TGATTT\",\n",
    "    post=\"\",\n",
    "    length=12\n",
    ")\n",
    "\n",
    "RT_UMI = finder.Barcode(\n",
    "    name=\"UMI\",\n",
    "    preceder=\"TGTCAC\",\n",
    "    post=\"\",\n",
    "    length=12\n",
    ")\n",
    "\n",
    "# Separate barcode objects\n",
    "AD_bc_objects = [AD, AD_BC]  # AD and AD_BC barcodes\n",
    "RT_bc_objects = [RT_BC]      # Reporter barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sequencing files\n",
    "trebl_AD_seq_files = glob.glob(\"path/to/AD_assembled/*\")  # Update this path\n",
    "trebl_RT_seq_files = glob.glob(\"path/to/RT_assembled/*\")  # Update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reads distribution for all files\n",
    "# NOTE: For large files, consider submitting this as a Savio job\n",
    "# See examples/savio_jobs/full_analysis_job.sh for job submission example\n",
    "\n",
    "pipeline.trebl_experiment_reads_distribution(\n",
    "    AD_seq_files=trebl_AD_seq_files,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_files=trebl_RT_seq_files,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True\n",
    ")\n",
    "# Generates histograms for all AD and RT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TREBL experiment with BOTH simple and directional/complex UMI deduplication\n",
    "trebl_results = pipeline.trebl_experiment_analysis(\n",
    "    AD_seq_files=trebl_AD_seq_files,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_files=trebl_RT_seq_files,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True,\n",
    "    step1_map_csv_path=\"output/full_analysis/step1_AD_AD_BC_RT_BC_error_corrected_designed.csv\",  # Update with your step1 CSV path\n",
    "    AD_umi_object=AD_UMI,\n",
    "    RT_umi_object=RT_UMI,\n",
    "    umi_deduplication='both'  # Use BOTH simple and directional/complex deduplication\n",
    ")\n",
    "\n",
    "# Access results\n",
    "# Results contain merged counts from both deduplication methods\n",
    "AD_results = trebl_results[\"AD_results\"]\n",
    "RT_results = trebl_results[\"RT_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "When `umi_deduplication='both'` is used:\n",
    "- Results include columns for both simple UMI counts and directional/complex UMI counts\n",
    "- The directional method typically gives lower counts as it removes UMIs that likely resulted from PCR/sequencing errors\n",
    "- For most analyses, the directional/complex counts are recommended for final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare simple vs complex deduplication results\n",
    "print(\"AD Results columns:\", AD_results.columns.tolist())\n",
    "print(\"\\nFirst few rows of AD results:\")\n",
    "AD_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After completing this full analysis:\n",
    "\n",
    "1. **Review outputs** in the `output/full_analysis` directory\n",
    "2. **Check loss tables** - You should see an additional 'error_corrected' step\n",
    "3. **Compare UMI deduplication methods** in the output CSVs\n",
    "4. **Calculate activity scores** using the results (see documentation)\n",
    "\n",
    "### Key Differences from Quick Start:\n",
    "\n",
    "- **Error correction** salvages more reads by correcting sequences similar to high-confidence barcodes\n",
    "- **Directional UMI deduplication** provides more accurate counts by handling UMI errors\n",
    "- **Processing time** is significantly longer but provides higher quality results\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "After analysis is complete, you can delete the DuckDB database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.remove(\"full_analysis.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trebl_env",
   "language": "python",
   "name": "trebl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}