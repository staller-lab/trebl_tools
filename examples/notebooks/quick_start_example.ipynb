{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREBL Quick Start Example\n",
    "\n",
    "This notebook demonstrates a quick start workflow for TREBL analysis with:\n",
    "- **No error correction** (faster processing)\n",
    "- **Simple UMI deduplication only** (for Step 1 - TREBL experiment)\n",
    "\n",
    "This is ideal for initial data exploration or when processing time is a priority.\n",
    "\n",
    "**Note:** For large files, the plotting steps (`step1_reads_distribution`, `trebl_experiment_reads_distribution`) can be computationally intensive and may benefit from submission as a Savio job instead of running interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trebl_tools import (\n",
    "    initial_map,\n",
    "    map_refiner,\n",
    "    complexity,\n",
    "    finder,\n",
    "    preprocess,\n",
    "    error_correct,\n",
    "    plotting,\n",
    "    umi_deduplicate,\n",
    "    pipelines\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pipeline\n",
    "\n",
    "Key settings for quick start:\n",
    "- `error_correction=False` - Skips error correction for faster processing\n",
    "- `test_n_reads` - Optional: Set to a number (e.g., 100000) for testing with subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline with no error correction\n",
    "pipeline = pipelines.TreblPipeline(\n",
    "    db_path=\"db/quick_start.db\",\n",
    "    design_file_path=None,  # No design file for this example\n",
    "    error_correction=False,  # No error correction for quick start\n",
    "    output_path=\"output/quick_start\"\n",
    "    # test_n_reads=100000  # Uncomment to test with first 100k reads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: TREBL Mapping\n",
    "\n",
    "Define barcodes and run initial mapping to establish barcode relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define barcodes to search for in reads\n",
    "AD = finder.Barcode(\n",
    "    name=\"AD\",\n",
    "    preceder=\"GGCTAGC\",\n",
    "    post=\"TGACTAG\",\n",
    "    length=120\n",
    ")\n",
    "\n",
    "AD_BC = finder.Barcode(\n",
    "    name=\"AD_BC\",\n",
    "    preceder=\"CGCGCC\",\n",
    "    post=\"GGGCCC\",\n",
    "    length=11\n",
    ")\n",
    "\n",
    "RT_BC = finder.Barcode(\n",
    "    name=\"RT_BC\",\n",
    "    preceder=\"CTCGAG\",\n",
    "    post=\"GGCCGC\",\n",
    "    length=14\n",
    ")\n",
    "\n",
    "# Combine barcodes\n",
    "bc_objects = [AD, AD_BC, RT_BC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify sequencing file(s)\n",
    "step1_seq_file = \"data/step1_ChopTFs_sample.fastq\"\n",
    "# Can be a single file (string) or multiple files (list of strings)\n",
    "# Supported formats: .fastq or .fastq.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reads distribution\n",
    "# NOTE: For large files (>10M reads), consider submitting this as a Savio job\n",
    "# See examples/savio_jobs/quick_start_job.sh for job submission example\n",
    "\n",
    "pipeline.step1_reads_distribution(\n",
    "    seq_file=step1_seq_file,\n",
    "    bc_objects=bc_objects,\n",
    "    reverse_complement=True\n",
    ")\n",
    "# Produces histogram of reads per barcode\n",
    "# Helps pick appropriate reads_threshold for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Step 1 mapping\n",
    "step1_map = pipeline.run_step_1(\n",
    "    seq_file=step1_seq_file,\n",
    "    bc_objects=bc_objects,\n",
    "    column_pairs=[(\"RT_BC\", \"AD\")],  # Check for collisions between RT_BC and AD\n",
    "    reads_threshold=10,  # Minimum reads to keep a barcode\n",
    "    reverse_complement=False\n",
    ")\n",
    "# Returns DataFrame of Step 1 mapping\n",
    "# Saves CSV, loss table visualization, and optional loss table CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: TREBL Step 2 Mapping\n",
    "\n",
    "Step 2 processes AD and RT libraries that are now in separate sequencing files.\n",
    "\n",
    "**Note:** Step 1 must be completed successfully before running Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing files for AD and RT (Step 2)\n",
    "step2_AD_seq_file = \"data/step2_ChopTFs_AD_sample.fastq\"\n",
    "step2_RT_seq_file = \"data/step2_ChopTFs_RT_sample.fastq\"\n",
    "# Can be single files or lists of files; .fastq or .fastq.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Step 2 reads distribution\n",
    "# NOTE: For large files (>10M reads), consider submitting this as a Savio job\n",
    "\n",
    "pipeline.step2_reads_distribution(\n",
    "    AD_seq_file=step2_AD_seq_file,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_file=step2_RT_seq_file,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True\n",
    ")\n",
    "# Produces histograms for AD and RT reads\n",
    "# Helps pick reads_threshold_AD and reads_threshold_RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Step 2 mapping\n",
    "step2 = pipeline.run_step_2(\n",
    "    AD_seq_file=step2_AD_seq_file,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_file=step2_RT_seq_file,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True,\n",
    "    reads_threshold_AD=10,\n",
    "    reads_threshold_RT=10,\n",
    "    step1_map_csv_path=\"output/quick_start/step1_AD_AD_BC_RT_BC.csv\"  # Update with your step1 CSV path\n",
    ")\n",
    "",
    "# Extract outputs\n",
    "AD_step2 = step2[\"AD_step2\"]\n",
    "RT_step2 = step2[\"RT_step2\"]\n",
    "step1_overlap = step2[\"step1_overlap\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREBL Experiment with Simple UMI Deduplication\n",
    "\n",
    "Process the full TREBL experiment using simple UMI deduplication only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UMI objects\n",
    "AD_UMI = finder.Barcode(\n",
    "    name=\"UMI\",\n",
    "    preceder=\"TGATTT\",\n",
    "    post=\"\",\n",
    "    length=12\n",
    ")\n",
    "\n",
    "RT_UMI = finder.Barcode(\n",
    "    name=\"UMI\",\n",
    "    preceder=\"TGTCAC\",\n",
    "    post=\"\",\n",
    "    length=12\n",
    ")\n",
    "\n",
    "# Separate barcode objects\n",
    "AD_bc_objects = [AD, AD_BC]  # AD and AD_BC barcodes\n",
    "RT_bc_objects = [RT_BC]      # Reporter barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sequencing files\n",
    "trebl_AD_seq_files = [\"data/trebl_experiment_ChopTFs_AD_10.fastq\", \"data/trebl_experiment_ChopTFs_AD_60.fastq\"]\n",
    "trebl_RT_seq_files = [\"data/trebl_experiment_ChopTFs_RT_10.fastq\", \"data/trebl_experiment_ChopTFs_RT_60.fastq\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reads distribution for all files\n",
    "# NOTE: For large files, consider submitting this as a Savio job\n",
    "# See examples/savio_jobs/quick_start_job.sh for job submission example\n",
    "\n",
    "pipeline.trebl_experiment_reads_distribution(\n",
    "    AD_seq_files=trebl_AD_seq_files,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_files=trebl_RT_seq_files,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True\n",
    ")\n",
    "# Generates histograms for all AD and RT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TREBL experiment with SIMPLE UMI deduplication only\n",
    "trebl_results = pipeline.trebl_experiment_analysis(\n",
    "    AD_seq_files=trebl_AD_seq_files,\n",
    "    AD_bc_objects=AD_bc_objects,\n",
    "    RT_seq_files=trebl_RT_seq_files,\n",
    "    RT_bc_objects=RT_bc_objects,\n",
    "    reverse_complement=True,\n",
    "    step1_map_csv_path=\"output/quick_start/step1_AD_AD_BC_RT_BC.csv\",  # Update with your step1 CSV path\n",
    "    AD_umi_object=AD_UMI,\n",
    "    RT_umi_object=RT_UMI,\n",
    "    umi_deduplication='simple'  # Use ONLY simple deduplication for quick start\n",
    ")\n",
    "",
    "# Access results\n",
    "AD_results = trebl_results[\"AD_results\"]\n",
    "RT_results = trebl_results[\"RT_results\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After completing this quick start analysis:\n",
    "\n",
    "1. **Review outputs** in the `output/quick_start` directory\n",
    "2. **Check loss tables** to understand filtering at each step\n",
    "3. **Validate results** by examining the CSV files\n",
    "4. **For more comprehensive analysis**, see the `full_analysis_example.ipynb` notebook which includes:\n",
    "   - Error correction for improved accuracy\n",
    "   - Both simple and directional/complex UMI deduplication\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "After analysis is complete, you can delete the DuckDB database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.remove(\"quick_start.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trebl_env",
   "language": "python",
   "name": "trebl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}